{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4aa8ec1-3e24-4225-bd80-63b35e651d78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# About\n",
    "Here, I build a Q&A LLM chain to read the book \"Time Machine\" and answer questions about the book.\n",
    "\n",
    "The LLM model I ued is `google/flan-t5-large` model (through Hugging Face)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1fa200-49f6-45f3-957c-56e5e33a756e",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a878284-f1c1-44c6-ba89-765530494b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain related \n",
    "from langchain.document_loaders import GutenbergLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce32fff-ab00-46c0-bd73-7868577ad372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "# put Huggingface API token in an py file (..config/token_access.py) and load it\n",
    "from config import token_access\n",
    "\n",
    "# set HUGGINGFACEHUB_API_TOKEN to my token_access\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] =token_access.token_access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d5415-4dac-4400-8772-d37c530a5ef5",
   "metadata": {},
   "source": [
    "### Variables that require changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e26cf-6e51-45b1-8c61-213948407eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------#\n",
    "# data related\n",
    "#----------------#\n",
    "# a variable name used to define specific path\n",
    "author=\"HGWells\"\n",
    "\n",
    "# book txt file link\n",
    "book_link =\"https://www.gutenberg.org/cache/epub/35/pg35.txt\"\n",
    "\n",
    "\n",
    "#----------------#\n",
    "# model related\n",
    "#----------------#\n",
    "#  embeddings\n",
    "embeddings_model_id =\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# llm model used for reading the book\n",
    "llm_model_id =\"google/flan-t5-large\"\n",
    "\n",
    "\n",
    "#----------------#\n",
    "# your project root path \n",
    "#----------------#\n",
    "main_Dir = \"../book-reader\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d61d3-19c6-4d91-9654-177dc77c66b5",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd600bf0-4748-4e8b-a40f-2aca75ea1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------#\n",
    "# data dir\n",
    "#----------------#\n",
    "data_Dir = os.path.join(main_Dir,\"data\")\n",
    "\n",
    "# embeddings dir\n",
    "embedding_string= f\"{embeddings_model_id}\".replace(\"/\", \"_\").replace(\"-\",\"_\")\n",
    "\n",
    "# hugging face llm pipeline model dir\n",
    "embedding_Dir = os.path.join(data_Dir,f\"{author}_{embedding_string}\")\n",
    "\n",
    "#----------------#\n",
    "# model dir\n",
    "#----------------#\n",
    "model_Dir= os.path.join(main_Dir,\"model\")\n",
    "cache_Dir = os.path.join(model_Dir,\"cache\")\n",
    "                           \n",
    "#----------------#\n",
    "# make dirs\n",
    "#----------------#\n",
    "for f in [data_Dir, embedding_Dir, model_Dir, cache_Dir]:\n",
    "    os.makedirs(f, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9addd-a9b1-4622-80a5-3783b262cb42",
   "metadata": {},
   "source": [
    "# Read a book \n",
    "### \"The Time Machine by H. G. Wells\" from Gutenberg Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a78bcd-5d47-448a-a93c-40055adea01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the book \n",
    "book_loader = GutenbergLoader(book_link)  \n",
    "\n",
    "document = book_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c429ade-59d4-4f04-80a7-fd18737b88be",
   "metadata": {},
   "source": [
    "## Chunking and Embeddings \n",
    " * split data into chunks using a CharacterTextSplitter\n",
    " * use Hugging Face's embedding LLM to embed this data for our vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d9bf550-3dad-4b54-b848-7a5c430b753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk sizes of 1024 and an overlap of 256 \n",
    "text_splitter = CharacterTextSplitter(chunk_size =1024, chunk_overlap=256) \n",
    "texts = text_splitter.split_documents(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1304a46b-9e64-4d7b-a554-39ac899e7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"sentence-transformers/all-MiniLM-L6-v2\" as model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name = embeddings_model_id,\n",
    "  cache_folder=cache_Dir\n",
    ")\n",
    "\n",
    "# Make Index using chromadb and the embeddings LLM\n",
    "chromadb_index = Chroma.from_documents(\n",
    "  texts,\n",
    "    embeddings,\n",
    "    persist_directory=embedding_Dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf3b7a-e97a-4e9c-80f9-4bc0334d6d7d",
   "metadata": {},
   "source": [
    "## Creating a QA LLM Chain\n",
    "This chain will be used to do QA on the document. We will need\n",
    " * A vector database that can perform document retrieval\n",
    " * An LLM to do the language interpretation\n",
    " * Specification on how to deal with this data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89c75bd2-1c75-45d5-a33c-79d3fa313b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████████████████████████████████| 2.54k/2.54k [00:00<00:00, 590kB/s]\n",
      "spiece.model: 100%|████████████████████████████████████████████████| 792k/792k [00:00<00:00, 4.84MB/s]\n",
      "tokenizer.json: 100%|████████████████████████████████████████████| 2.42M/2.42M [00:00<00:00, 5.25MB/s]\n",
      "special_tokens_map.json: 100%|████████████████████████████████████| 2.20k/2.20k [00:00<00:00, 501kB/s]\n",
      "config.json: 100%|███████████████████████████████████████████████████| 662/662 [00:00<00:00, 56.9kB/s]\n",
      "model.safetensors: 100%|█████████████████████████████████████████| 3.13G/3.13G [07:43<00:00, 6.75MB/s]\n",
      "/Users/j/miniconda3/envs/llm/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "generation_config.json: 100%|████████████████████████████████████████| 147/147 [00:00<00:00, 13.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert index to retriever (A wrapper around the functionality of our vector database so we can search for similar documents/chunks in the vectorstore and retrieve the results)\n",
    "retriever = chromadb_index.as_retriever()\n",
    "\n",
    "# llm\n",
    "hf_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=llm_model_id,\n",
    "    task=\"text2text-generation\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_length\": 128,\n",
    "        \"cache_dir\": cache_Dir,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cccd88-5297-4aba-9a5d-6afb229b6dbe",
   "metadata": {},
   "source": [
    "### Options: stuff, map_reduce, refine, map_rerank\n",
    "* stuff - Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model.\n",
    " * map_reduce - This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk).\n",
    " * refine - This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document.\n",
    " * map_rerank - This method involves running an initial prompt on each chunk of data, that not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02e6f786-6627-428d-9fc1-3e5ffebf3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa retriever\n",
    "chain_type = \"stuff\" \n",
    "book_qa = RetrievalQA.from_chain_type(\n",
    "    llm=hf_llm, \n",
    "    chain_type=chain_type, \n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498c74f-6fe3-4a06-8508-3f51026adc04",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab80e3-72d8-4392-bbaf-35476852bdb8",
   "metadata": {},
   "source": [
    "### Did ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85dfe08a-cf90-441b-96b2-18b13af3b125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H. G. Wells'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who is the author of Time Machine?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b4771f2-c579-467f-9158-9f5c0ecf4f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Time Traveller'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who invented the Time Machine?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a8483f9-1f4f-4087-ae92-69e1bf3d6e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Eloi, like the Carlovignan kings, had decayed to a mere beautiful futility'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How do Eloi look like?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f4cd3e2-1546-4e35-8924-afc9944dc9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'half-bleached colour of the worms'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How do Morlock look like?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e54a03e-ccb0-4858-be9f-f8302c6efd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'another way of looking at Time'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"In this novella, what is the fourth dimension?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a4d6bfb-8555-452e-82e5-3e81a3adbd21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a fence of fire'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What temporarily blinds the Morlocks?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e44a3-99c9-4b0c-8dcb-10788f0e3a16",
   "metadata": {},
   "source": [
    "### Did not do well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a94af221-6054-4a9c-bbf3-0d9a8f093fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who wrote this book?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "510b9cbb-9db0-4c6f-8d57-1395d0327a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Time Traveller'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who is Filby?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d585232-0371-463c-9866-49b631575a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'velocity'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What scientific principle does the Time Traveler's visit to the decaying world thirty millions in the future illustrate?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6129f22e-2ebf-4ad1-a754-8a1c4f1cce6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the workshop'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Where is the Time Machine held?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2354d1de-2b37-4230-af56-2ae62c54a662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meat'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What do the Eloi eat?\"\n",
    "answer = book_qa.run(question)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d87729-ceec-441f-908e-8b93e4681cfb",
   "metadata": {},
   "source": [
    "# Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a93216-189a-41b4-956e-94f4fc7b7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hugging face llm pipeline\n",
    "hf_llm_string= f\"{llm_model_id}\".replace(\"/\", \"_\").replace(\"-\",\"_\")\n",
    "\n",
    "# hugging face llm pipeline model dir\n",
    "hf_llm_Dir = os.path.join(model_Dir,f\"{author}_{hf_llm_string}\")\n",
    "\n",
    "# create dir if not existing \n",
    "for f in [hf_llm_Dir]:\n",
    "    os.makedirs(f, exist_ok=True)\n",
    "\n",
    "\n",
    "# save huggingface pipeline \n",
    "hf_llm.save(os.path.join(hf_llm_Dir,f\"{hf_llm_string}.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
